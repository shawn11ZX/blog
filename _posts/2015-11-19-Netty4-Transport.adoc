---
layout: post
title: Netty4 Transport
shortTitle: Netty4 Transport
---

:toc: macro
:toclevels: 4
:sectnums:
:imagesdir: /images
:hp-tags: TLS, SSL, MAC
:doctypes: book

toc::[]


///////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////

== EventLoop

image:netty-eventLoop.png[]

=== 相关类

* 方框内是interface，方框外是class
* 浅蓝色是java.util.concurrent包中的executor
* 绿色是 io.netty.util.concurrent包中的EventExecutor
* 黄色是 io.netty.util.concurrent包中的EventLoop

Executor相关类主要负责（立即或者定时）执行Runnable/Callable；EventExecutor类增加了判断当前线程是否在EventLoop中的方法；EventLoop相关类提供接口将Channel register/unregister到一个EventLoop，另外返回一个返回ChannelHandlerInvoker用于在EventLoop触发register/active/write等行为或事件

///////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////


[width="100%",options="header,footer"]
|====================
| 类 | 提示 
| Executor | 执行Runnable 
| ExecutorService | 执行Callable;  shutdown 
| ScheduledExecutorService  | 定时执行Runnable和Callable 
| EventExecutor | 判断当前线程是否在本executor：inEventLoop 
| EventExecutorGroup | shudown; EventExector集合 
| EventLoop | 返回ChannelHandlerInvoker，用于在EventLoop触发register,active, write等行为或事件 
| EventLoopGroup | EventLoop集合;register channel; 返回EventExecutor（next函数） 
|====================

=== NioEventLoop相关实现

类NioEventLoop实现了EventLoop，但其具体的实现通过上层的类提供： AbstractScheduledEventExecutor和SingleThreadEventExecutor和SingleThreadEventLoop。

.AbstractScheduledEventExecutor

* 实现schedule Runnable/Callable功能；
* 有一个scheduledTaskQueue，用PriorityQueue实现，头部是到期时间最早的task

.SingleThreadEventExecutor

基于AbstractScheduledEventExecutor提供了addTask和pollTask的功能。

拥有两个queue：

* 本身有一个taskQueue，用来保存马上需要执行的任务
* 继承自AbstractScheduledEventExecutor的scheduledTaskQueue

函数takeTask用来获取一个需要马上执行的runnable，函数的实现如下：

* 查看scheduledTaskQueue头，如果没有任何scheduled task，则从taskQueue中take(无限等待）。

[NOTE] 
====
可能会感觉这里有bug，即在无限等待过程中，新加入scheduled tastk queue的task不会被执行。事实是在向其提交ScheduledTask时，会做以下操作：
[source,java]
----
 <V> ScheduledFuture<V> schedule(final ScheduledFutureTask<V> task) {
        if (inEventLoop()) { // (1)
            scheduledTaskQueue().add(task);
        } else { // (2)
            execute(new OneTimeTask() {
                @Override
                public void run() {
                    scheduledTaskQueue().add(task);
                }
            });
        }

        return task;
    }
----

- 在（1）情况下，下一个循环又会先检查scheduledTaskQueue，所以会被执行
- 在（2）情况下，线程会先被唤醒执行OneTimeTask，下一个循环还是会去先scheduledTaskQueue

因此是没有bug的

====

* 如果有scheduled task，则检查dueTime。
** 如果已经到期，则返回。
** 如果没有到期，从等待taskQueue，一直到dueTime。
*** 如果dueTime到了taskQueue还没有东西，则返回scheduled task。
*** 否则返回taskQueue里的东西。

boolean runAllTasks(long timeoutNanos) 会被子类用到，执行task直到timeout。

.SingleThreadEventLoop

这个类没啥东西

.NioEventLoop本身

依靠Java NIO（如Selector），提供了register channel的相关功能(中间需要调用Channel.register）。同时，override SingleThreadEventExecutor中的run函数，从而定义线程主循环。

主循环逻辑如下：

* select并等待唤醒(这里逻辑复杂，看不懂）
* 根据设置的io/task执行时间比例，按比例执select到的任务和task任务

select到的任务主要指socket的read/write，task任务主要指提交的Runnable和Callable

///////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////
== Channel

=== 相关类

image:netty-channel.png[]

=== NioServerSocketChannel
在使用ServerBootStrap时，需要提供一个Channel类，用来处理server socket的read操作（即accept）。使用NIO + TCP时，一般使用NioServerSocketChannel。

很奇怪的是，该类继承AbstractNioMessageChannel，这是因为AbstractNioMessageChannel实现了read函数(确切的说是实现了AbstractNioUnsafe子类），并会要求子类实现一个readMessage。而NioServerSocketChannel.readMessage则通过调用accept返回一个NioSocketChannel对象。



=== AbstractChannel

* 包含了register,write,bind等实现框架
* 需要子类去实现doRegister等函数，例如AbstractNioChannel实现了doRegister, NioSocketChannel实现了doWrite等

image:netty-AbstractChannel-do.png[]


== Pipeline

image:netty-pipeline.png[]

* DefaultChannelPipeLien包含了以AbstractChannelHandlerContext为元素的双线链表。该双线链表的Head和Tail都是内建不能更改的。当我们向pipeline增加ChannelHandler时（如调用addLast），实际上是创建一个AbstractChannelHandlerContext并添加到这个双向链表中。

* event类消息，从head开始向下遍历，比如fireChannelRegistered，fireChannelActive，fireChannelRead等

* action类消息，从tail开始遍历，比如write，read，connect，close等

* 内建的HeadContext会处理connect，write等调用，并调用AbstractChannel中相应的函数。具体的处理细节根据action和channel的类型都会不同。

* 内建的TailContext没什么特别处理，只对没人处理的ByteBuf做释放操作

== AddressResolver

Netty中的AddressResolver负责将名字翻译成SocketAddress。为了提高速度(Or somthing else)，Netty定义了AddressResolverGroup，每个EventExecutor关联一个Resolver

Netty有三种AddressResolverGroup实现:

* DefaultAddressResolverGroup: 通过Java自带的InetAddress.getByName来解析
* DnsAddressResolverGroup：Netty自己实现的一套DNS解析，有Cache功能
* NoopAddressResolverGroup: 啥都不干


在调用BootStrap.connect()时，会在当前线程调用AddressResolver.resolve(remoteAddress)，而默认的实现DefaultAddressResolverGroup这个操作是 **阻塞** 的！

DnsAddressResolverGroup实现没看(貌似是4.1新出的功能)，不过应该不会是阻塞的(但是也会在Channel的EventGroup上执行)。构造的时候需要：

* EventLoop
* name server list


///////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////
== 相关流程

下面的相关流程特指NioServerSocketChannel/NioSocketChannel

==== bind流程

* 新建一个Channel
* 注册到boss group，注册时触发事件
** fireChannelRegistered
** fireChannelActive(如果是第一次注册，例如unregister后再register没有这个事件）
* 调用Java NIO bind到端口

如下图：　

* Channel为NioServerChannel或相关子类中的实现
* EventLoopGroup为NioEventLoopGroup或相关子类中的实现
* EventLoop为NioEventLoop或相关子类中的实现

image:ServerBootStrap_bind.png[]

[plantuml,ServerBootStrap_bind,png,width="100%"]
----
@startuml


BootStrap -> BootStrap: doBind
group 
    BootStrap -> BootStrap: initAndRegister
    BootStrap -> ChannelFactory: newChannel
    BootStrap -> EventLoopGroup: register(channel)
    EventLoopGroup -> EventLoopGroup: next
    EventLoopGroup -> EventLoop: register
    EventLoop -> Channel: register(this, promise)
    Channel -> Channel: 根据inEventLoop同步或异步register0; 设置this.eventLoop; 调用子类doRegister
    Channel -> SelectableChannel: register(java nio实现)
    Channel -> ChannelPipeline: callHandlerAdded0触发ctx.handler().handlerAdded(ctx)
    Channel -> ChannelPipeline: fireChannelRegistered
    BootStrap -> BootStrap: doBind0
    BootStrap -> Channel: 在Channel的EventLoop中调用bind，触发ChannelHandlerInvoker.invokeBindNow
end 
@enduml
----



==== accept流程

ServerBootstrap继承自AbstractBootStrap，前者在构造时需要两个EventLoopGroup：boss和worker
而后者在构造时，只需要一个EventLoopGroup。这是因为ServerBootstrap把需要bind和listen的Channel让父类处理，处理的逻辑定义在ServerBootstrapAcceptor：

* 实现ChannelInboundHandlerAdapter
* 当channelRead时（有新的Channel时），将childHander应用到该Channel上
* 在worker group上注册这个新的Channel

==== connect流程

* 新建一个Channel
* 注册到boss group，注册时触发事件
** fireChannelRegistered
** fireChannelActive(如果是第一次注册，例如unregister后再register没有这个事件）
* resolve name
* 调用channel.connect




==== write流程

* 用户调用Channel.write

* 调用Pipeline.tail（TailContext）的write，默认实现是查找下一个个Outbound Context，并根据是否在eventloop，同步或异步的调用ChannelOutboundHandler.write函数

* 我们定义一个ChannelOutboundHandler时，默认在最后也会调用context.write，因此又继续调用下一层

* 最终，会调用到Pipeline.head（HeadContext）的write，默认实现时调用AbstractChannel中的write。

* AbstractChannel的write默认是放到一个ChannelOutboundBuffer，只有flush时才真正write（细节由channel实现）


=== read流程

* 读的发起地是NioEventLoop，在处理SelectionKey.OP_READ时发起。首先调用Channel的read（不同channel不同的实现）

* channel的read中，一般读取网络数据后，调用Pipeline.fireChannelRead

* 如上所述，Pipeline.fireChannelRead从head开始，向上查找Inbound Channel Handler，然后调用其invokeChannelRead

* invokeChannelRead会根据是否在event loop中，同步或异步的调用channelRead函数，也就是我们一般会去override函数

